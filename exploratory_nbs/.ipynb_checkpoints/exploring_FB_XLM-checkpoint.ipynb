{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch model from [here](https://github.com/facebookresearch/XLM/blob/master/README.md#ii-cross-lingual-language-model-pretraining-xlm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FAISS library was not found.\n",
      "FAISS not available. Switching to standard nearest neighbors search implementation.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "from XLM.src.utils import AttrDict\n",
    "from XLM.src.data.dictionary import Dictionary, BOS_WORD, EOS_WORD, PAD_WORD, UNK_WORD, MASK_WORD\n",
    "from XLM.src.model.transformer import TransformerModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = 'XLM/models/mlm_100_1280.pth'\n",
    "reloaded = torch.load(model_path, map_location=torch.device('cpu'))\n",
    "params = AttrDict(reloaded['params'])\n",
    "\n",
    "# build dictionary / update parameters\n",
    "dico = Dictionary(reloaded['dico_id2word'], reloaded['dico_word2id'], reloaded['dico_counts'])\n",
    "params.n_words = len(dico)\n",
    "params.bos_index = dico.index(BOS_WORD)\n",
    "params.eos_index = dico.index(EOS_WORD)\n",
    "params.pad_index = dico.index(PAD_WORD)\n",
    "params.unk_index = dico.index(UNK_WORD)\n",
    "params.mask_index = dico.index(MASK_WORD)\n",
    "\n",
    "# build model / reload weights\n",
    "model = TransformerModel(params, dico, True, True)\n",
    "model.eval()\n",
    "model.load_state_dict(reloaded['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerModel(\n",
       "  (position_embeddings): Embedding(512, 1280)\n",
       "  (embeddings): Embedding(200000, 1280, padding_idx=2)\n",
       "  (layer_norm_emb): LayerNorm((1280,), eps=1e-12, elementwise_affine=True)\n",
       "  (attentions): ModuleList(\n",
       "    (0): MultiHeadAttention(\n",
       "      (q_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      (k_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      (v_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      (out_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "    )\n",
       "    (1): MultiHeadAttention(\n",
       "      (q_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      (k_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      (v_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      (out_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "    )\n",
       "    (2): MultiHeadAttention(\n",
       "      (q_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      (k_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      (v_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      (out_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "    )\n",
       "    (3): MultiHeadAttention(\n",
       "      (q_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      (k_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      (v_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      (out_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "    )\n",
       "    (4): MultiHeadAttention(\n",
       "      (q_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      (k_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      (v_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      (out_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "    )\n",
       "    (5): MultiHeadAttention(\n",
       "      (q_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      (k_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      (v_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      (out_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "    )\n",
       "    (6): MultiHeadAttention(\n",
       "      (q_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      (k_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      (v_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      (out_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "    )\n",
       "    (7): MultiHeadAttention(\n",
       "      (q_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      (k_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      (v_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      (out_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "    )\n",
       "    (8): MultiHeadAttention(\n",
       "      (q_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      (k_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      (v_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      (out_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "    )\n",
       "    (9): MultiHeadAttention(\n",
       "      (q_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      (k_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      (v_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      (out_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "    )\n",
       "    (10): MultiHeadAttention(\n",
       "      (q_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      (k_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      (v_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      (out_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "    )\n",
       "    (11): MultiHeadAttention(\n",
       "      (q_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      (k_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      (v_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      (out_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "    )\n",
       "    (12): MultiHeadAttention(\n",
       "      (q_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      (k_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      (v_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      (out_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "    )\n",
       "    (13): MultiHeadAttention(\n",
       "      (q_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      (k_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      (v_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      (out_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "    )\n",
       "    (14): MultiHeadAttention(\n",
       "      (q_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      (k_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      (v_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      (out_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "    )\n",
       "    (15): MultiHeadAttention(\n",
       "      (q_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      (k_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      (v_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      (out_lin): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (layer_norm1): ModuleList(\n",
       "    (0): LayerNorm((1280,), eps=1e-12, elementwise_affine=True)\n",
       "    (1): LayerNorm((1280,), eps=1e-12, elementwise_affine=True)\n",
       "    (2): LayerNorm((1280,), eps=1e-12, elementwise_affine=True)\n",
       "    (3): LayerNorm((1280,), eps=1e-12, elementwise_affine=True)\n",
       "    (4): LayerNorm((1280,), eps=1e-12, elementwise_affine=True)\n",
       "    (5): LayerNorm((1280,), eps=1e-12, elementwise_affine=True)\n",
       "    (6): LayerNorm((1280,), eps=1e-12, elementwise_affine=True)\n",
       "    (7): LayerNorm((1280,), eps=1e-12, elementwise_affine=True)\n",
       "    (8): LayerNorm((1280,), eps=1e-12, elementwise_affine=True)\n",
       "    (9): LayerNorm((1280,), eps=1e-12, elementwise_affine=True)\n",
       "    (10): LayerNorm((1280,), eps=1e-12, elementwise_affine=True)\n",
       "    (11): LayerNorm((1280,), eps=1e-12, elementwise_affine=True)\n",
       "    (12): LayerNorm((1280,), eps=1e-12, elementwise_affine=True)\n",
       "    (13): LayerNorm((1280,), eps=1e-12, elementwise_affine=True)\n",
       "    (14): LayerNorm((1280,), eps=1e-12, elementwise_affine=True)\n",
       "    (15): LayerNorm((1280,), eps=1e-12, elementwise_affine=True)\n",
       "  )\n",
       "  (ffns): ModuleList(\n",
       "    (0): TransformerFFN(\n",
       "      (lin1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "      (lin2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "    )\n",
       "    (1): TransformerFFN(\n",
       "      (lin1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "      (lin2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "    )\n",
       "    (2): TransformerFFN(\n",
       "      (lin1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "      (lin2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "    )\n",
       "    (3): TransformerFFN(\n",
       "      (lin1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "      (lin2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "    )\n",
       "    (4): TransformerFFN(\n",
       "      (lin1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "      (lin2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "    )\n",
       "    (5): TransformerFFN(\n",
       "      (lin1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "      (lin2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "    )\n",
       "    (6): TransformerFFN(\n",
       "      (lin1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "      (lin2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "    )\n",
       "    (7): TransformerFFN(\n",
       "      (lin1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "      (lin2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "    )\n",
       "    (8): TransformerFFN(\n",
       "      (lin1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "      (lin2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "    )\n",
       "    (9): TransformerFFN(\n",
       "      (lin1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "      (lin2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "    )\n",
       "    (10): TransformerFFN(\n",
       "      (lin1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "      (lin2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "    )\n",
       "    (11): TransformerFFN(\n",
       "      (lin1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "      (lin2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "    )\n",
       "    (12): TransformerFFN(\n",
       "      (lin1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "      (lin2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "    )\n",
       "    (13): TransformerFFN(\n",
       "      (lin1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "      (lin2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "    )\n",
       "    (14): TransformerFFN(\n",
       "      (lin1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "      (lin2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "    )\n",
       "    (15): TransformerFFN(\n",
       "      (lin1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "      (lin2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (layer_norm2): ModuleList(\n",
       "    (0): LayerNorm((1280,), eps=1e-12, elementwise_affine=True)\n",
       "    (1): LayerNorm((1280,), eps=1e-12, elementwise_affine=True)\n",
       "    (2): LayerNorm((1280,), eps=1e-12, elementwise_affine=True)\n",
       "    (3): LayerNorm((1280,), eps=1e-12, elementwise_affine=True)\n",
       "    (4): LayerNorm((1280,), eps=1e-12, elementwise_affine=True)\n",
       "    (5): LayerNorm((1280,), eps=1e-12, elementwise_affine=True)\n",
       "    (6): LayerNorm((1280,), eps=1e-12, elementwise_affine=True)\n",
       "    (7): LayerNorm((1280,), eps=1e-12, elementwise_affine=True)\n",
       "    (8): LayerNorm((1280,), eps=1e-12, elementwise_affine=True)\n",
       "    (9): LayerNorm((1280,), eps=1e-12, elementwise_affine=True)\n",
       "    (10): LayerNorm((1280,), eps=1e-12, elementwise_affine=True)\n",
       "    (11): LayerNorm((1280,), eps=1e-12, elementwise_affine=True)\n",
       "    (12): LayerNorm((1280,), eps=1e-12, elementwise_affine=True)\n",
       "    (13): LayerNorm((1280,), eps=1e-12, elementwise_affine=True)\n",
       "    (14): LayerNorm((1280,), eps=1e-12, elementwise_affine=True)\n",
       "    (15): LayerNorm((1280,), eps=1e-12, elementwise_affine=True)\n",
       "  )\n",
       "  (memories): ModuleDict()\n",
       "  (pred_layer): PredLayer(\n",
       "    (proj): Linear(in_features=1280, out_features=200000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes = os.path.join(os.getcwd(), 'XLM/codes_xnli_100.txt')\n",
    "fastbpe = os.path.join(os.getcwd(), 'XLM/tools/fastBPE/fast')\n",
    "\n",
    "def to_bpe(sentences):\n",
    "    # write sentences to tmp file\n",
    "    with open('/tmp/sentences', 'w') as fwrite:\n",
    "        for sent in sentences:\n",
    "            fwrite.write(sent + '\\n')\n",
    "    \n",
    "    # apply bpe to tmp file\n",
    "    os.system('%s applybpe /tmp/sentences.bpe /tmp/sentences %s' % (fastbpe, codes))\n",
    "    \n",
    "    # load bpe-ized sentences\n",
    "    sentences_bpe = []\n",
    "    with open('/tmp/sentences.bpe') as f:\n",
    "        for line in f:\n",
    "            sentences_bpe.append(line.rstrip())\n",
    "    \n",
    "    return sentences_bpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_example_sentences(path):\n",
    "    \n",
    "    with open(path, 'r') as fread:\n",
    "        sents_raw = fread.readlines()[:350]\n",
    "\n",
    "    return sents_raw\n",
    "        \n",
    "sents = load_example_sentences('../data/en/CONLL2003/train.txt')\n",
    "\n",
    "# Remove POS tags and other unnecessary formatting\n",
    "sents = ' '.join([s.split(' ')[0] for s in sents_raw[1:] if s != '\\n']).split('.')\n",
    "sents = [sent.strip() + '.' for sent in sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['EU re@@ jects German call to bo@@ yc@@ ott British lam@@ b.',\n",
       " 'Peter Blackburn BR@@ US@@ SE@@ LS 1996-@@ 0@@ 8-@@ 22 The European Commission said on Thur@@ sday it dis@@ agreed with German ad@@ vice to consum@@ ers to sh@@ un British lam@@ b until scientists deter@@ mine whether mad cow disease can be transmit@@ ted to she@@ ep@@ .',\n",
       " \"Germany 's represent@@ ative to the European Union 's veterin@@ ary committee Werner Zw@@ ing@@ mann said on Wed@@ nes@@ day consum@@ ers should buy she@@ ep@@ meat from countries other than Britain until the scientific ad@@ vice was cle@@ ar@@ er@@ .\",\n",
       " '\" We do n@@ \\'t support any such recommen@@ d@@ ation because we do n@@ \\'t see any gro@@ unds for it , \" the Commission \\'s chief spo@@ kes@@ man Nikolaus van der Pas told a news brief@@ ing@@ .',\n",
       " 'He said further scientific study was required and if it was found that action was needed it should be taken by the European Union@@ .',\n",
       " 'He said a propos@@ al last month by EU Farm Com@@ missioner Franz Fisch@@ ler to ban she@@ ep bra@@ ins , sp@@ le@@ ens and sp@@ inal c@@ ords from the human and animal food ch@@ ains was a highly specific and pre@@ cau@@ tionary move to protect human heal@@ th@@ .',\n",
       " 'Fisch@@ ler proposed EU-@@ wide meas@@ ures after reports from Britain and France that under labor@@ atory conditions she@@ ep could contract Bov@@ ine Spon@@ gi@@ form En@@ cephal@@ opat@@ hy ( B@@ SE ) -- mad cow dise@@ as@@ e.',\n",
       " \"But Fisch@@ ler agreed to review his propos@@ al after the EU 's standing veterin@@ ary committee , m@@ ational animal health officials , question@@ ed if such action was justi@@ fied as there was only a s@@ light risk to human heal@@ th@@ .\",\n",
       " 'Spanish Farm Minister Lo@@ yola de Palacio had earlier accu@@ sed Fisch@@ ler at an EU farm ministers \\' meeting of causing un@@ justi@@ fied alar@@ m through \" dangerous gener@@ alis@@ ation@@ .',\n",
       " '\".',\n",
       " \"Only France and Britain back@@ ed Fisch@@ ler 's propos@@ al.\",\n",
       " \"The EU 's scientific veterin@@ ary and multi@@ disciplin@@ ary commit@@ tees are due to re-@@ ex@@ amine the issue early next month and make recommen@@ d@@ ations to the senior veterin@@ ary offici@@ al@@ s.\",\n",
       " 'She@@ ep have long been known to contract sc@@ rap@@ ie , a bra@@ in-@@ wast@@ ing disease similar to B@@ SE which is believed to have been transferred to cat@@ tle through feed containing animal wast@@ e.',\n",
       " 'British far@@ mers den@@ ied@@ .']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_bpe(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Но@@ вым мини@@ стром по дел@@ ам Bre@@ xit стал До@@ ми@@ ник Ра@@ аб@@ , который замени@@ л на этом по@@ сту уш@@ ед@@ шего на@@ кан@@ у@@ не ве@@ че@@ ром в отстав@@ ку Дэ@@ вида Дэ@@ ви@@ са@@ .',\n",
       " ' Ран@@ ее Ра@@ аб возглав@@ лял министер@@ ство по дел@@ ам жи@@ ли@@ щ@@ ного строительства Великобрита@@ ни@@ и@@ , а также был замест@@ ителем министра ю@@ сти@@ ци@@ и@@ .',\n",
       " ' Се@@ год@@ ня же стало извест@@ но@@ , что коро@@ лева Великобритании одоб@@ ри@@ ла его на@@ значение на новую долж@@ ност@@ ь@@ .',\n",
       " ' На данный момент это самы@@ й ответ@@ ственный пост в правитель@@ стве@@ , вед@@ ь со@@ всем скоро стране пред@@ стоит вы@@ ход из Евро@@ со@@ ю@@ за@@ .',\n",
       " ' Э@@ кс@@ пер@@ ты уже окрест@@ или на@@ значение До@@ ми@@ ника Ра@@ а@@ ба на новую должность прояв@@ лением от@@ ча@@ я@@ ния со стороны премьер-@@ министра@@ .',\n",
       " ' От@@ мет@@ им@@ , что тот же Дэ@@ вис покин@@ ул свой пост из-за не@@ прия@@ тия под@@ хода Те@@ рез@@ ы М@@ эй в перегово@@ рах с Е@@ С.',\n",
       " ' По его м@@ не@@ ни@@ ю@@ , она сли@@ шком легко согла@@ ша@@ ется на все условия Евро@@ со@@ ю@@ за@@ , за@@ бы@@ вая об интере@@ сах своей стра@@ ны@@ .',\n",
       " '',\n",
       " '.']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ru_sents_raw = load_example_sentences('../data/ru/raw/ru/brexit_ru.txt_file_1032.txt')\n",
    "to_bpe([s + '.' for s in ru_sents_raw[-1].split('.') if s])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
